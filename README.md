# RecipeGen-LLM: Fine-Tuned Recipe Generation with Llama 3B

A complete end-to-end pipeline for generating personalized recipes using **Llama 3.2 3B Instruct** fine-tuned with **LoRA** on synthetic recipe data.

## ğŸ¯ Project Overview

This project demonstrates:
- **Synthetic data generation** using Groq API (Llama 3.1 8B)
- **Dietary constraint validation** and data cleaning
- **LoRA fine-tuning** on Lambda Labs GPU cluster
- **Full-stack web application** with base vs fine-tuned model comparison
- **Deployment-ready** architecture (local + future GCP)

## ğŸ“Š Key Results

- **12,000 synthetic recipes** generated across 6 diverse scenarios
- **100% dietary constraint compliance** after validation and cleaning
- **Fine-tuned model** trained on Lambda Labs (Llama 3.2 3B + LoRA)
- **Web app** with real-time recipe generation and inventory management

---

## ğŸ—ï¸ Project Structure

```
RecipeGen-LLM/
â”œâ”€â”€ data_pipeline/                 # Data generation and training pipeline
â”‚   â”œâ”€â”€ 01_synthetic_generation/   # Groq API recipe generation
â”‚   â”œâ”€â”€ 02_chat_conversion/        # Convert to Llama 3 chat format
â”‚   â”œâ”€â”€ 03_validation/             # Dietary constraint validation
â”‚   â””â”€â”€ 04_training/               # Lambda Labs fine-tuning
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llama3b_lambda_lora/       # Trained LoRA adapter (35MB)
â”œâ”€â”€ backend/                       # FastAPI + PyTorch model service
â”œâ”€â”€ frontend/                      # React web application
â”œâ”€â”€ evaluation/                    # Model testing and comparison
â””â”€â”€ deployment/                    # Docker and deployment configs
```

---

## ğŸ“– Complete Pipeline

### 1. Synthetic Data Generation (Groq API)

**Why 6 scenarios?**

We designed 6 scenarios to cover real-world recipe generation use cases:

1. **Scenario 1 (3,000)**: Full inventory usage
   - Cultural-specific (Italian, Chinese, Mexican, etc.)
   - Neutral everyday ingredients
   - Fusion cuisines

2. **Scenario 2 (2,400)**: Dietary constraints
   - **Vegan** (800): No animal products
   - **Vegetarian** (800): No meat/fish
   - **Gluten-free** (400): No wheat/gluten
   - **Dairy-free** (400): No milk/cheese

3. **Scenario 3 (1,800)**: Cuisine-specific
   - Italian, Chinese, Mexican, Indian, Japanese, Korean (300 each)

4. **Scenario 4 (1,200)**: Combined constraints
   - Cuisine + dietary preference (e.g., vegan Thai, gluten-free Italian)

5. **Scenario 5 (2,400)**: User-requested ingredients
   - All requested ingredients available in inventory
   - Tests ingredient prioritization

6. **Scenario 6 (1,200)**: Missing ingredients
   - Partial match (900): Some requested ingredients unavailable
   - No match (300): All requested ingredients missing
   - Tests fallback behavior and suggestions

**Total**: 12,000 recipes

**Technology**: Groq API with **Llama 3.1 8B**
- **Why Groq?** 10-100x faster than OpenAI GPUs, cost-effective for high-volume generation
- **Why Llama 3.1 8B?** Native JSON output, strong instruction following, open source

See [`data_pipeline/01_synthetic_generation/SCENARIOS.md`](data_pipeline/01_synthetic_generation/SCENARIOS.md) for detailed scenario descriptions.

### 2. Chat Format Conversion

Converts structured JSON to **ChatML format**:

```
<|im_start|>system
You are a recipe generation AI that creates recipes based on user inventory and preferences.<|im_end|>
<|im_start|>user
I have tofu, rice, vegetables. I want a vegan recipe.<|im_end|>
<|im_start|>assistant
{
  "status": "ok",
  "missing_ingredients": [],
  "recipe": {
    "name": "Vegan Tofu Fried Rice",
    "cuisine": "Asian",
    "culinary_preference": "vegan",
    "time": "20m",
    "main_ingredients": ["tofu", "rice", "vegetables"],
    "steps": "1. Press tofu and cut into cubes...\n2. Heat oil in wok...",
    "note": "Add soy sauce to taste"
  },
  "shopping_list": []
}<|im_end|>
```

**Why ChatML format?**
- Matches training paradigm of instruction-tuned models
- Explicit role separation (system/user/assistant)
- Structured JSON output for easy parsing and display
- Better constraint enforcement through system prompts
- Standard format for Hugging Face trainers

### 3. Data Validation and Cleaning

**Validation process**:
1. **Dietary constraint checking**
   - Vegan: No meat, dairy, eggs, honey
   - Vegetarian: No meat, fish
   - Gluten-free: No wheat, flour, barley, rye
   - Dairy-free: No milk, butter, cheese

2. **Violation detection**
   - Pattern matching for forbidden ingredients
   - Context-aware checking (e.g., "almond milk" â‰  violation for vegan)

3. **Data cleaning**
   - Removed ~150 recipes with violations (1.25%)
   - Final dataset: **11,850 clean recipes**
   - 100% dietary compliance verified

**Common violations found**:
- Vegan recipes with honey (45 cases)
- Gluten-free recipes with soy sauce (38 cases)
- Dairy-free recipes with butter (32 cases)
- Vegetarian recipes with chicken broth (35 cases)

### 4. Fine-Tuning on Lambda Labs

**Setup**:
- Platform: **Lambda Labs GPU Cloud**
- Instance: 1x A100 (40GB VRAM)
- Duration: ~3 hours
- Cost: ~$15

**Model Configuration**:
- Base model: `meta-llama/Llama-3.2-3B-Instruct`
- Fine-tuning method: **LoRA (Low-Rank Adaptation)**
  - Rank (r): 16
  - Alpha: 32
  - Target modules: q_proj, k_proj, v_proj, o_proj
  - Dropout: 0.05

**Training Hyperparameters**:
- Learning rate: 2e-4
- Batch size: 4 (per device)
- Gradient accumulation: 4 steps
- Epochs: 3
- Optimizer: AdamW (8-bit)
- LR scheduler: Cosine with warmup

**Why LoRA?**
- **Efficient**: Only trains 0.1% of parameters (~35MB adapter vs 6GB full model)
- **Fast**: 3 hours vs days for full fine-tuning
- **Portable**: Easy to share and deploy
- **Memory efficient**: Fits on consumer GPUs

**Training Dataset Split**:
- Train: 9,480 recipes (80%)
- Validation: 1,185 recipes (10%)
- Test: 1,185 recipes (10%)

See [`data_pipeline/04_training/LAMBDA_LABS_SETUP_GUIDE.md`](data_pipeline/04_training/LAMBDA_LABS_SETUP_GUIDE.md) for detailed setup instructions.

### 5. Model Evaluation

**Test Results**:
- Dietary constraint compliance: **100%** (5/5 test cases passed)
- Base model vs Fine-tuned: Significant improvement in constraint adherence
- Response quality: More structured recipes, better ingredient utilization

**Test Categories**:
1. Vegetarian with dairy âœ…
2. Vegan (no animal products) âœ…
3. Gluten-free âœ…
4. Dairy-free âœ…
5. Multiple constraints (vegan + gluten-free) âœ…

See [`evaluation/reports/lambda_model_test_results.json`](evaluation/reports/lambda_model_test_results.json) for detailed results.

### 6. Deployment: Full-Stack Web Application

**Architecture**:
```
Frontend (React) â†â†’ Backend (FastAPI) â†â†’ Model Service (PyTorch + LoRA)
                                      â†“
                                 MongoDB (Inventory + Preferences)
```

**Features**:
- **Recipe Generation**: Natural language requests with inventory
- **Base vs Fine-tuned Comparison**: Side-by-side model outputs
- **Inventory Management**: Track pantry items with quantities
- **Dietary Preferences**: Set and persist dietary constraints
- **Cuisine Preferences**: Request specific cuisine styles

**Technology Stack**:

**Backend**:
- FastAPI (async Python web framework)
- PyTorch (ML inference)
- Transformers + PEFT (model loading)
- MongoDB (async with Motor)

**Frontend**:
- React 18
- Axios (API client)
- CSS modules

**Model Serving**:
- Device: MPS (Apple Silicon) / CUDA / CPU auto-detection
- Model: Llama 3.2 3B Instruct (6GB base + 35MB LoRA)
- Inference: ~2-5 seconds per recipe

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10+
- Node.js 16+
- Docker (for MongoDB)
- 8GB+ RAM

### 1. Clone Repository

```bash
git clone https://github.com/Shinhunjun/recipe-generation-smallllm.git
cd recipe-generation-smallllm
```

### 2. Download Model and Data Files

**Important**: The trained model and training data are not included in the Git repository due to file size. Download them from Google Drive:

ğŸ“¦ **Model Files** (52 MB):
- Download: [llama3b_lambda_lora.zip](https://drive.google.com/file/d/1gPWh_ap_OLzseJui477wcmtwjNVOa0XA/view?usp=drive_link)
- Extract to: `models/`

```bash
# After downloading, extract:
unzip llama3b_lambda_lora.zip -d models/
```

ğŸ“Š **Training Data** (45 MB):
- Download: [data.zip](https://drive.google.com/file/d/1S2FmttufCx9OJ5D8LG4G0JRv4in6Fv-1/view?usp=drive_link)
- Extract to: `data_pipeline/data/`

```bash
# After downloading, extract:
unzip data.zip -d data_pipeline/data/
```

**Expected directory structure after extraction**:
```
RecipeGen-LLM/
â”œâ”€â”€ models/
â”‚   â””â”€â”€ llama3b_lambda_lora/
â”‚       â”œâ”€â”€ adapter_model.safetensors
â”‚       â”œâ”€â”€ adapter_config.json
â”‚       â””â”€â”€ ...
â””â”€â”€ data_pipeline/data/
    â”œâ”€â”€ chat_format/
    â”‚   â”œâ”€â”€ recipes_train_chat.jsonl
    â”‚   â”œâ”€â”€ recipes_val_chat.jsonl
    â”‚   â””â”€â”€ recipes_test_chat.jsonl
    â”œâ”€â”€ cleaned/
    â”‚   â”œâ”€â”€ recipes_train_cleaned.jsonl
    â”‚   â”œâ”€â”€ recipes_val_cleaned.jsonl
    â”‚   â””â”€â”€ recipes_test_cleaned.jsonl
    â””â”€â”€ synthetic/
        â””â”€â”€ recipes_15k_raw.jsonl
```

### 3. Start MongoDB

```bash
docker run -d -p 27017:27017 --name mongodb mongo:latest
```

### 4. Backend Setup

```bash
cd backend

# Create and activate virtual environment
python3 -m venv venv
source venv/bin/activate  # macOS/Linux
# venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Start server
python main.py
```

Server runs on `http://localhost:8000`

### 5. Frontend Setup (New Terminal)

```bash
cd frontend

# Install dependencies
npm install

# Start development server
npm start
```

App opens at `http://localhost:3000`

**For detailed setup instructions, see [SETUP.md](SETUP.md)**

### 6. Usage

1. **Add Inventory**: Go to "Inventory" tab, add ingredients
2. **Set Preferences**: Go to "Settings" tab, set dietary constraints
3. **Generate Recipe**:
   - Enter natural language request (e.g., "Make me a vegan pasta dish")
   - Toggle "Compare models" to see base vs fine-tuned outputs
   - Click "Generate Recipe"

---

## ğŸ“¦ Model Weights

The fine-tuned LoRA adapter is included in `models/llama3b_lambda_lora/`:
- `adapter_model.safetensors` (35MB)
- `adapter_config.json`
- Tokenizer files

**Base model** (`meta-llama/Llama-3.2-3B-Instruct`) is auto-downloaded from Hugging Face on first run.

---

## ğŸ”¬ Evaluation

Run comprehensive dietary constraint tests:

```bash
cd evaluation
python test_lambda_model.py
```

This tests:
- 5 dietary constraint scenarios
- Base vs fine-tuned comparison
- Violation detection
- Recipe quality metrics

Results saved to `evaluation/reports/lambda_model_test_results.json`

---

## ğŸŒ Future Deployment (GCP)

Planned architecture for cloud deployment:

```
Cloud Load Balancer
    â†“
Cloud Run (Frontend) â†â†’ Cloud Run (Backend API)
                              â†“
                         Vertex AI Model Endpoint
                              â†“
                         Cloud Storage (Model)
    â†“
MongoDB Atlas / Cloud SQL
```

**Benefits**:
- Auto-scaling
- Managed infrastructure
- Global CDN
- Serverless pricing

See [`deployment/`](deployment/) for Docker configs.

---

## ğŸ“š Documentation

- **Data Pipeline**: [`data_pipeline/`](data_pipeline/)
  - [01_synthetic_generation](data_pipeline/01_synthetic_generation/README.md)
  - [02_chat_conversion](data_pipeline/02_chat_conversion/README.md)
  - [03_validation](data_pipeline/03_validation/README.md)
  - [04_training](data_pipeline/04_training/LAMBDA_LABS_SETUP_GUIDE.md)

- **Scenarios**: [SCENARIOS.md](data_pipeline/01_synthetic_generation/SCENARIOS.md)

- **Model Architecture**: [model_service.py](backend/model_service.py)

---

## ğŸ› ï¸ Technology Choices

### Why Llama 3.2 3B?
- **Size**: Small enough for local inference (6GB), large enough for quality
- **Instruction-tuned**: Pre-trained for chat format
- **Open source**: No API costs, full control
- **Apple Silicon optimized**: Fast inference on MPS

### Why LoRA?
- **Parameter-efficient**: 35MB adapter vs 6GB full model
- **Fast training**: 3 hours vs days
- **Easy deployment**: Swap adapters without reloading base model
- **Research-proven**: PEFT library by Hugging Face

### Why Groq API?
- **Speed**: 10-100x faster than OpenAI for synthetic data generation
- **Cost**: Lower pricing for high-volume API calls
- **Quality**: Llama 3.1 8B comparable to GPT-3.5

### Why FastAPI?
- **Async**: Non-blocking I/O for model inference
- **Type safety**: Pydantic models
- **OpenAPI**: Auto-generated docs
- **Performance**: Faster than Flask/Django

### Why React?
- **Component-based**: Reusable UI components
- **Ecosystem**: Rich libraries (Axios, React Router)
- **Developer experience**: Hot reload, debugging tools

---

## ğŸ“ License

MIT License

---

## ğŸ‘¥ Contributing

Contributions welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Submit a pull request

---

## ğŸ™ Acknowledgments

- **Groq** for fast and affordable LLM API
- **Lambda Labs** for GPU cloud infrastructure
- **Hugging Face** for Transformers and PEFT libraries
- **Meta** for open-sourcing Llama 3 models

---

## ğŸ“§ Contact

For questions or feedback, please open an issue on GitHub.

---

**Built with â¤ï¸ using Llama 3, PyTorch, FastAPI, and React**
